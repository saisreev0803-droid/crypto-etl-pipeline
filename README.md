# ğŸš€ Crypto ETL Pipeline with BigQuery

A production-style **Data Engineering ETL pipeline** that extracts cryptocurrency market data from an external API, transforms it into analytics-ready format, and loads it into **Google BigQuery** for cloud-based analysis.

This project simulates how real-world data pipelines are designed to handle ingestion, transformation, storage, and warehouse loading.

---

## ğŸ¯ Project Objective

Build an automated data pipeline that:

- Ingests cryptocurrency prices from a public API  
- Stores raw source data for traceability  
- Transforms nested JSON into structured tabular data  
- Loads clean historical data into a cloud warehouse  

This mirrors how companies build **daily ingestion pipelines** for analytics systems.

---

## ğŸ§  Skills Demonstrated

- API Data Ingestion  
- ETL Pipeline Design  
- JSON Processing  
- Data Modeling  
- Cloud Data Warehousing  
- Schema Handling  
- Data Lineage Concepts  

---

## ğŸ— Pipeline Architecture

**Extract â†’ Transform â†’ Load (ETL)**

| Stage | Description |
|------|-------------|
| **Extract** | Python script calls CoinGecko API |
| **Raw Layer** | Saves immutable JSON data |
| **Transform** | Converts JSON into structured dataset |
| **Clean Layer** | Stores analytics-ready CSV |
| **Load** | Appends data into BigQuery warehouse |

---

## ğŸ“‚ Project Structure

crypto-etl-pipeline/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ etl_extract.py 
â”‚ â”œâ”€â”€ etl_transform.py 
â”‚ â””â”€â”€ etl_load_bigquery.py 
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ 
â”‚ â””â”€â”€ clean/ 
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ“Š Data Warehouse Table Schema

| Column | Description |
|--------|------------|
| `date_utc` | Reference date |
| `coin` | Cryptocurrency name |
| `currency` | Fiat currency (USD) |
| `price` | Market price |
| `source_updated_at_utc` | Timestamp from source API |
| `ingested_at_utc` | Pipeline ingestion time |

---

## âš™ï¸ How to Run the Pipeline

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
2ï¸âƒ£ Extract API Data
python src/etl_extract.py
3ï¸âƒ£ Transform Raw Data
python src/etl_transform.py
4ï¸âƒ£ Load into BigQuery
python src/etl_load_bigquery.py
â˜ï¸ Cloud Configuration
This project uses Google BigQuery as the cloud data warehouse.

Set credentials before loading:

set GOOGLE_APPLICATION_CREDENTIALS=your-key-file.json
